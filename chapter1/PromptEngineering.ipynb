{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd2ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baebdc9",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443805e9",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993cb3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a few ways to translate \"How are you?\" into French, depending on the level of formality:\n",
      "\n",
      "*   **Formal:** **Comment allez-vous ?** (This is the most formal and polite option, using the formal \"vous\" form.)\n",
      "\n",
      "*   **Informal:** **Comment vas-tu ?** (This is used with friends, family, or people you know well, using the informal \"tu\" form.)\n",
      "\n",
      "*   **Very informal:** **Ça va ?** (This is a very common and casual way to ask, similar to \"You okay?\" or \"What's up?\")\n",
      "\n",
      "*   **Common, slightly less informal than \"Ça va ?\":** **Comment ça va ?** (This is also very common and can be used in many situations, but it's slightly less informal than just \"Ça va ?\")\n",
      "\n",
      "So, the best translation depends on the context. If you're unsure, **\"Comment allez-vous ?\"** is always a safe bet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {sentence}\")\n",
    "response = model.invoke(prompt.format(sentence=\"How are you?\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f38f5",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299118b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Translate English to French:\n",
    "- Hello → Bonjour\n",
    "- Good night → Bonne nuit\n",
    "- {text} →\"\"\")\n",
    "\n",
    "response = model.invoke(prompt.format(text=\"Thank you\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b18f1",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5bce6",
   "metadata": {},
   "source": [
    "#### Zero-shot-CoT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02f1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break it down:\n",
      "\n",
      "*   **Start:** Sarah begins with 12 apples.\n",
      "*   **Gives away:** She gives away 4 apples, so we subtract: 12 - 4 = 8 apples.\n",
      "*   **Buys more:** She buys 7 more apples, so we add: 8 + 7 = 15 apples.\n",
      "\n",
      "**Answer:** Sarah has 15 apples now.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Solve the following problem.\n",
    "\n",
    "Question: Sarah has 12 apples. She gives away 4, then buys 7 more.\n",
    "How many apples does she have now?\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "\"\"\")\n",
    "\n",
    "response = model.invoke(prompt.format())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bb9d5",
   "metadata": {},
   "source": [
    "#### Few-shot-CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0550db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Start with 12.\n",
      "Step 2: Subtract 4 → 12 - 4 = 8\n",
      "Step 3: Add 7 → 8 + 7 = 15\n",
      "Answer: 15\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Solve each of the following problems step-by-step.\n",
    "\n",
    "Q: Tom had 10 candies. He ate 3 and bought 2 more. How many candies does he have now?  \n",
    "A: Step 1: Start with 10.  \n",
    "Step 2: Subtract 3 → 10 - 3 = 7  \n",
    "Step 3: Add 2 → 7 + 2 = 9  \n",
    "Answer: 9\n",
    "\n",
    "Q: Sarah has 12 apples. She gives 4 away, then buys 7 more. How many apples does she have now?  \n",
    "A:\"\"\")\n",
    "\n",
    "response = model.invoke(prompt.format())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f108a",
   "metadata": {},
   "source": [
    "### Tree-of-Thought (ToT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0159687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's do that.\n",
      "\n",
      "Multiples of 4: 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100\n",
      "\n",
      "Now, let's check which of these are also divisible by 6:\n",
      "\n",
      "*   12 / 6 = 2\n",
      "*   24 / 6 = 4\n",
      "*   36 / 6 = 6\n",
      "*   48 / 6 = 8\n",
      "*   60 / 6 = 10\n",
      "*   72 / 6 = 12\n",
      "*   84 / 6 = 14\n",
      "*   96 / 6 = 16\n",
      "\n",
      "So, the numbers between 1 and 100 that are divisible by both 4 and 6 are: 12, 24, 36, 48, 60, 72, 84, and 96.\n",
      "\n",
      "The smallest of these is 12. So, a number between 1 and 100 that is divisible by both 4 and 6 is 12.\n",
      "\n",
      "Answer: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the problem\n",
    "question = \"Find a number between 1 and 100 that is divisible by both 4 and 6.\"\n",
    "\n",
    "# Generate multiple reasoning paths\n",
    "thought_prompts = [\n",
    "    f\"Let's solve it this way: Start listing numbers divisible by 4.\",\n",
    "    f\"Try a different path: List numbers divisible by 6 first.\",\n",
    "    f\"Try finding the Least Common Multiple (LCM) of 4 and 6.\"\n",
    "]\n",
    "\n",
    "# Collect candidate answers\n",
    "candidates = []\n",
    "for thought in thought_prompts:\n",
    "    response = model.invoke(f\"{question}\\n{thought}\")\n",
    "    candidates.append(response.content)\n",
    "\n",
    "# Evaluate answers (simple heuristic: check if response includes correct number)\n",
    "final_answers = [ans for ans in candidates if \"12\" in ans or \"divisible by 12\" in ans]\n",
    "\n",
    "# Choose the best one\n",
    "best = final_answers[0] if final_answers else \"No clear answer found\"\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ce344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Step 1: Generate Possible Solutions\n",
    "prompt1 = PromptTemplate.from_template(\"\"\"\n",
    "You're helping someone reduce their daily screen time.\n",
    "\n",
    "Step 1:\n",
    "Come up with three different strategies they could try. Be practical and consider different types of solutions (behavioral, environmental, or technical).\n",
    "\n",
    "A:\"\"\")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"ideas\")\n",
    "\n",
    "# Step 2: Evaluate Each Strategy\n",
    "prompt2 = PromptTemplate.from_template(\"\"\"\n",
    "Step 2:\n",
    "Evaluate the pros and cons of each of the following strategies to reduce screen time:\n",
    "\n",
    "{ideas}\n",
    "\n",
    "Include ease of implementation, effectiveness, and sustainability in your evaluation.\n",
    "\n",
    "A:\"\"\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"evaluations\")\n",
    "\n",
    "# Step 3: Choose the Best Strategy\n",
    "prompt3 = PromptTemplate.from_template(\"\"\"\n",
    "Step 3:\n",
    "Based on the evaluations, pick the most promising strategy for reducing screen time. Justify your choice in a few sentences.\n",
    "\n",
    "{evaluations}\n",
    "\n",
    "A:\"\"\")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt3, output_key=\"final_choice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7c9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Based on the evaluations, I recommend starting with the \"Screen-Free Zone & Designated Device\" approach.** This strategy offers a good balance of ease of implementation, effectiveness, and sustainability. It's relatively simple to set up, doesn't rely heavily on willpower, and can create a more mindful environment that naturally reduces screen time exposure. While it might not be a complete solution, it provides a solid foundation for further adjustments and the potential integration of other strategies if needed.\n"
     ]
    }
   ],
   "source": [
    "# Combine into a Sequential Chain\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3],\n",
    "    input_variables=[],\n",
    "    output_variables=[\"final_choice\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = tot_chain({})\n",
    "print(response[\"final_choice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ea701",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0,\n",
    "    # max_tokens=1024 \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc33ca-28ba-496d-8544-defb4b32f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads the .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f96c91-9990-4275-b51b-37ba0e4082e2",
   "metadata": {},
   "source": [
    "### PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbc7090-f3ff-4832-9df0-62d73f693f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Tell me a joke about cats'\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# fill the values of the placeholders\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202b676-a093-47f5-a98f-14043880b4f0",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9d6487-fece-4b70-82cb-9a3455c3944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is Googly', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'cricket','topic':'Googly'})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ceda8-8b97-4557-b045-721d7efbe22d",
   "metadata": {},
   "source": [
    "### Message Placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af12a205-f1c7-4c04-8230-12675ee0f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, your order #435 has been prepared for shipping and will be delivered within 3-5 business days. This means you should receive your order between Monday and Wednesday next week, assuming today is a business day. Would you like me to provide you with the tracking information once it becomes available?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_history = []\n",
    "# load chat history\n",
    "with open('chat_history.txt') as f:\n",
    "    chat_history.extend(f.readlines())\n",
    "    \n",
    "# chat template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system','You are a helpful customer support agent'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human','{query}')\n",
    "])\n",
    "\n",
    "# create prompt\n",
    "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'When can I expect my order?'})\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c9304-7afb-407a-bb49-e8663dc7bfc6",
   "metadata": {},
   "source": [
    "### Few-Shot Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c246bd4-1ff2-4d71-80e1-868990835ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Give a synonym for each word:\n",
      "\n",
      "Word: hello\n",
      "Synonym: hi\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Synonym: joyful\n",
      "\n",
      "\n",
      "Word: sad\n",
      "Synonym:\n",
      "--------------------------------------------------\n",
      "Output: Word: sad\n",
      "Synonym: unhappy\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"hello\", \"synonym\": \"hi\"},\n",
    "    {\"word\": \"happy\", \"synonym\": \"joyful\"}\n",
    "]\n",
    "\n",
    "example_template = PromptTemplate.from_template(\"Word: {word}\\nSynonym: {synonym}\\n\")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Give a synonym for each word:\",\n",
    "    suffix=\"Word: {input}\\nSynonym:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format(input=\"sad\")\n",
    "output = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"Prompt:\\n\", formatted_prompt)\n",
    "print(\"-\"*50)\n",
    "print(\"Output:\", output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98981154-6795-4eee-b659-86072f5368c3",
   "metadata": {},
   "source": [
    "### Message Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3e7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "# System message for context\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert {field} consultant with {years} years of experience.\"\n",
    ")\n",
    "\n",
    "# Human message for the query\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"I need help with {problem}. My current situation is: {context}\"\n",
    ")\n",
    "\n",
    "# AI message for providing examples of responses\n",
    "ai_prompt = AIMessagePromptTemplate.from_template(\n",
    "    \"I understand you need help with {problem}. Let me analyze your situation.\"\n",
    ")\n",
    "\n",
    "# Combine all messages\n",
    "full_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    human_prompt,\n",
    "    ai_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b670a40-d68f-4994-92f7-fa4cd2e5541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSAGES: [SystemMessage(content='You are an expert marketing consultant with 10 years of experience.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I need help with low conversion rates on my website. My current situation is: We're getting a lot of traffic, but very few visitors are signing up for our product.\", additional_kwargs={}, response_metadata={}), AIMessage(content='I understand you need help with low conversion rates on my website. Let me analyze your situation.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Provide input values for all placeholders\n",
    "formatted_prompt = full_prompt.invoke({\n",
    "    \"field\": \"marketing\",\n",
    "    \"years\": \"10\",\n",
    "    \"problem\": \"low conversion rates on my website\",\n",
    "    \"context\": \"We're getting a lot of traffic, but very few visitors are signing up for our product.\"\n",
    "})\n",
    "\n",
    "# Print each message from the formatted prompt\n",
    "for role, content in formatted_prompt:\n",
    "    print(f\"{role.upper()}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd16f2-b8ad-45fb-8053-744b17277721",
   "metadata": {},
   "source": [
    "### Partial Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a8cbe2-90c3-4fb7-b415-62da04fccdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a senior developer helping with code review in web development\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"You are a {role} helping with {task} in {domain}\",\n",
    "    input_variables=[\"role\", \"task\", \"domain\"]\n",
    ")\n",
    "\n",
    "# Partially fill the prompt\n",
    "partial_prompt = prompt.partial(role=\"senior developer\")\n",
    "\n",
    "# Later, complete the remaining variables\n",
    "final_prompt = partial_prompt.format(\n",
    "    task=\"code review\",\n",
    "    domain=\"web development\"\n",
    ")\n",
    "\n",
    "# Output the final prompt\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d81b8-ca8d-4761-acc2-cf624dce5b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a8b14-d35d-44e6-9573-bbbcb81bb415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ad2c5-8568-4e6c-afb0-e12e26a99604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada98d15-4258-472b-b63b-ccd2eb45a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "llm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
