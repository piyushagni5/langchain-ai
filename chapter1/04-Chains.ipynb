{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122e73fa",
   "metadata": {},
   "source": [
    "### Load the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f25d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from langchain_aws import ChatBedrock\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b0a3b",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f723e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for this notebook, we use the Google Generative AI model\n",
    "# Uncomment the following line to use Anthropic's Claude model instead or another model\n",
    "\n",
    "# llm = ChatAnthropic(model=\"claude-3.5-sonnet-20240620\", anthropic_api_key=\"...\")\n",
    "# llm = ChatBedrock( model_id=anthropic.claude-3-5-sonnet-20241022-v2:0, region_name=\"us-west-2\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b3a22",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045cb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden leaves are falling down,\n",
      "Crisp air whispers through the town,\n",
      "Nature's beauty wears a crown.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Write a 3 line poem on {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "result = chain.invoke({\"topic\": \"autumn\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00edda7",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85427e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 2-point summary of the provided text:\n",
      "\n",
      "*   **Global recessions are significant declines in worldwide economic activity, characterized by widespread contractions in GDP, trade, investment, and employment, often triggered by financial crises, demand/supply shocks, policy mistakes, or structural imbalances.**\n",
      "*   **Effective policy responses, including coordinated monetary and fiscal measures, financial sector reforms, and international cooperation, are crucial to mitigating the impacts of global recessions and promoting a sustainable recovery, especially given the current economic climate of high inflation, geopolitical tensions, and supply chain disruptions.**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a 2-point summary from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt1 | llm | parser | prompt2 | llm | parser\n",
    "result = chain.invoke({'topic': 'Global Recession'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f30d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae13b0d",
   "metadata": {},
   "source": [
    "### Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a969bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great, memorable way to present the information! The summary is concise and accurate, and the fun fact is engaging and humorous. The \"plant farts\" analogy is certainly attention-grabbing and makes the concept of oxygen being a byproduct of photosynthesis much more memorable. Well done!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "# Initialize model and parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1a: Summary prompt\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize this topic:\\n{text}\")\n",
    "summary_chain = summary_prompt | llm | parser\n",
    "\n",
    "# Step 1b: Fun fact prompt\n",
    "fact_prompt = PromptTemplate.from_template(\"Give a fun fact about:\\n{text}\")\n",
    "fact_chain = fact_prompt | llm | parser\n",
    "\n",
    "# Run both in parallel\n",
    "parallel = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"fact\": fact_chain\n",
    "})\n",
    "\n",
    "# Step 2: Merge both outputs\n",
    "merge_prompt = PromptTemplate.from_template(\n",
    "    \"Here is a short summary and a fun fact combined:\\nSummary: {summary}\\nFun Fact: {fact}\"\n",
    ")\n",
    "merge_chain = merge_prompt | llm | parser\n",
    "\n",
    "# Final full chain\n",
    "chain = parallel | merge_chain\n",
    "\n",
    "# Sample input\n",
    "text = \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water.\"\n",
    "\n",
    "# Run\n",
    "result = chain.invoke({\"text\": text})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2fd60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-----------------------------+                    \n",
      "                    | Parallel<summary,fact>Input |                    \n",
      "                    +-----------------------------+                    \n",
      "                       ***                   ***                       \n",
      "                   ****                         ****                   \n",
      "                 **                                 **                 \n",
      "    +----------------+                          +----------------+     \n",
      "    | PromptTemplate |                          | PromptTemplate |     \n",
      "    +----------------+                          +----------------+     \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "+------------------------+                  +------------------------+ \n",
      "| ChatGoogleGenerativeAI |                  | ChatGoogleGenerativeAI | \n",
      "+------------------------+                  +------------------------+ \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "    +-----------------+                         +-----------------+    \n",
      "    | StrOutputParser |                         | StrOutputParser |    \n",
      "    +-----------------+                         +-----------------+    \n",
      "                       ***                   ***                       \n",
      "                          ****           ****                          \n",
      "                              **       **                              \n",
      "                   +------------------------------+                    \n",
      "                   | Parallel<summary,fact>Output |                    \n",
      "                   +------------------------------+                    \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +----------------+                           \n",
      "                          | PromptTemplate |                           \n",
      "                          +----------------+                           \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +------------------------+                       \n",
      "                      | ChatGoogleGenerativeAI |                       \n",
      "                      +------------------------+                       \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +-----------------+                          \n",
      "                          | StrOutputParser |                          \n",
      "                          +-----------------+                          \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +-----------------------+                        \n",
      "                      | StrOutputParserOutput |                        \n",
      "                      +-----------------------+                        \n"
     ]
    }
   ],
   "source": [
    "# Visualize the chain\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7c3ae",
   "metadata": {},
   "source": [
    "### Conditional Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd977bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand. I'm very sorry to hear that you had a negative experience. I truly value your feedback, as it helps me learn and improve. Could you please provide me with more details about what went wrong? Knowing the specifics will allow me to address the issue and prevent it from happening again in the future.\n",
      "\n",
      "In the meantime, please accept my sincere apologies. I am committed to doing better.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "text_parser = StrOutputParser()\n",
    "# 1. Define structured output schema\n",
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(\n",
    "        description=\"Classify the feedback sentiment\"\n",
    "    )\n",
    "structured_parser = PydanticOutputParser(pydantic_object=Feedback)\n",
    "# 2. Classification prompt (with parser instruction)\n",
    "classification_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Analyze the feedback and classify it as 'positive' or 'negative'.\\n\"\n",
    "        \"Feedback: {feedback}\\n\\n\"\n",
    "        \"{format_instruction}\"\n",
    "    ),\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instruction\": structured_parser.get_format_instructions()}\n",
    ")\n",
    "# Chain: classify → structured output\n",
    "classifier_chain = classification_prompt | llm | structured_parser\n",
    "# 3. Conditional branches for response generation\n",
    "prompt1 = PromptTemplate.from_template(\n",
    "          \"Write a warm thank-you message for this feedback:\\n{feedback}\"\n",
    "           )\n",
    "positive_response = prompt1 | llm | text_parser\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "          \"Write a polite and helpful apology response for this feedback:\\n{feedback}\"\n",
    "          )\n",
    "negative_response = prompt2 | llm | text_parser\n",
    "# 4. Conditional logic using RunnableBranch\n",
    "response_branch = RunnableBranch(\n",
    "    (lambda x: x.sentiment == \"positive\", positive_response),\n",
    "    (lambda x: x.sentiment == \"negative\", negative_response),\n",
    "    RunnableLambda(lambda _: \"Sentiment could not be classified.\")\n",
    ")\n",
    "# 5. Final chain: classify → choose response\n",
    "chain = classifier_chain | response_branch\n",
    "# Example usage\n",
    "feedback_text = \"The product arrived late and the box was damaged.\"\n",
    "print(chain.invoke({\"feedback\": feedback_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "llm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
